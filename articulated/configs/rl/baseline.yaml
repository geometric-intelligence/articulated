# Team RL: Baseline configuration (raw observations)
# Usage: python -m articulated.rl.train --config articulated/configs/rl/baseline.yaml

seed: 42

agent:
  algorithm: "ppo"
  use_embedding: false
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  clip_range: 0.2
  device: "cpu"
  tensorboard_log: "logs/rl"
  normalize_observations: true
  normalize_rewards: false
  clip_obs: 10.0

training:
  total_timesteps: 500000
  eval_freq: 50000
  save_path: "checkpoints/rl/baseline_ppo"
